# -*- coding: utf-8 -*-
"""
extract_body.py
----------------
å¾ OCR æ–‡å­—æª”ï¼ˆ.txtï¼‰æŠ½å–è‡ºç£åˆ¶å¼å…¬æ–‡æ¬„ä½ï¼Œé‡é»æ“·å–æ­£æ–‡ï¼ˆå…§æ–‡ï¼‰ã€‚
æ”¯æ´æ‰¹æ¬¡è™•ç†ï¼šéæ­· input_root ä¸‹æ‰€æœ‰å­è³‡æ–™å¤¾èˆ‡ .txt æª”ï¼Œè¼¸å‡ºåˆ° output_rootï¼ˆæ¯é¡åˆ¥ä¸€å€‹ .jsonlï¼‰ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
  åŸºæœ¬ï¼š
    python extract_body.py
  æŒ‡å®šè¼¸å…¥/è¼¸å‡ºè³‡æ–™å¤¾ï¼š
    python extract_body.py --input-root output --output-root parsed
  åƒ…è™•ç†ç‰¹å®šå­è³‡æ–™å¤¾ï¼ˆå¯å¤šå€‹ï¼‰ï¼š
    python extract_body.py --folders ä¿å–®æŸ¥è©¢ é€šçŸ¥å‡½
  åƒ…è™•ç†æª”ååŒ…å«é—œéµå­—çš„ .txtï¼š
    python extract_body.py --filename-contains è¨»è¨˜

è¼¸å‡ºï¼š
  parsed/ä¿å–®æŸ¥è©¢.jsonl
  parsed/é€šçŸ¥å‡½.jsonl
  ...
æ¯è¡Œä¸€ä»½æ–‡ä»¶çš„ JSON çµæœï¼ˆUTF-8, ensure_ascii=Falseï¼‰
"""

import argparse
import json
import os
import re
from pathlib import Path
from typing import Dict, Tuple, List, Optional

# =========================
# 1) æ¬„ä½åˆ¥åèˆ‡å®¹éŒ¯æ­£è¦åŒ–
# =========================

FIELD_ALIASES = {
    "recipient":  ["å—æ–‡è€…", "å—æ–‡æ©Ÿé—œ", "å—æ–‡å–®ä½"],
    "doc_no":     ["ç™¼æ–‡å­—è™Ÿ", "æ–‡è™Ÿ", "æ¡ˆè™Ÿ", "ä¾†æ–‡å­—è™Ÿ"],
    "date":       ["ç™¼æ–‡æ—¥æœŸ", "æ—¥æœŸ", "ä¸­è¯æ°‘åœ‹"],
    "priority":   ["é€Ÿåˆ¥"],
    "security":   ["å¯†ç­‰"],
    "subject":    ["ä¸»æ—¨"],
    "body":       ["èªªæ˜", "å…§æ–‡", "æ­£æ–‡", "æœ¬æ–‡"],
    "attachment": ["é™„ä»¶"],
    "cc":         ["å‰¯æœ¬", "æ­£æœ¬", "æŠ„é€"],
    "contact":    ["æ‰¿è¾¦", "æ‰¿è¾¦äºº", "è¯çµ¡", "è¯çµ¡é›»è©±", "é€£çµ¡é›»è©±"],
}

# OCR å¸¸è¦‹éŒ¯å­— / å…¨åŠå½¢ / å†’è™Ÿ / åˆ†éš”ç¬¦ æ­£è¦åŒ–
CANONICAL_REPLACEMENTS = [
    # å¸¸è¦‹èª¤è¾¨
    ("ç‹æ—¨", "ä¸»æ—¨"), ("åœ­æ—¨", "ä¸»æ—¨"),
    ("èªªæœ‹", "èªªæ˜"), ("èªªçœ€", "èªªæ˜"),
    # å†’è™Ÿèˆ‡ç©ºç™½
    ("ï¼š", ":"), ("ï¹•", ":"), ("ï¸°", ":"), ("ï¼š", ":"),
    ("ã€€", " "), ("ï»¿", ""), ("\ufeff", ""),
    # ç ´æŠ˜è™Ÿé¡
    ("ï¼", "-"), ("â€”", "-"), ("â€“", "-"),
]


def normalize_text(raw: str) -> str:
    """ä¸€èˆ¬æ¸…æ´—ï¼šéŒ¯å­—æ›¿æ›ã€æ›è¡Œçµ±ä¸€ã€é çœ‰/é è…³/è¡Œè™Ÿ/åˆ†éš”ç·šæ¸…ç†ã€‚"""
    text = raw
    for a, b in CANONICAL_REPLACEMENTS:
        text = text.replace(a, b)

    text = text.replace("\r\n", "\n").replace("\r", "\n")

    lines = text.split("\n")
    cleaned = []
    for ln in lines:
        s = ln.strip()
        # å»é™¤åªæœ‰é ç¢¼æˆ–åˆ†éš”ç·šçš„è¡Œ
        if re.fullmatch(r"-{3,}|_{3,}|=+|~+|\d+/\d+|ç¬¬\d+é ", s):
            continue
        # å»æ‰æ¯è¡Œé–‹é ­è¡Œè™Ÿï¼ˆè‹¥æœ‰ï¼‰
        s = re.sub(r"^\s*\(?\d{1,3}\)?\s+", "", s)
        cleaned.append(s)
    text = "\n".join(cleaned)
    return text


def build_field_regex() -> re.Pattern:
    """å‹•æ…‹å»º regex ä»¥åµæ¸¬æ¬„ä½åè¡Œï¼šæ”¯æ´ã€æ¬„ä½: å…§å®¹ã€æˆ–ä¸‹ä¸€è¡Œé–‹å§‹å…§å®¹ã€‚"""
    names: List[str] = []
    for _, alias in FIELD_ALIASES.items():
        names.extend(alias)
    names = sorted(set(names), key=lambda x: -len(x))  # é•·è©å„ªå…ˆ

    pattern = r"^(?P<field>(" + "|".join(map(re.escape, names)) + r"))\s*:?\s*(?P<after>.*)$"
    return re.compile(pattern, flags=re.MULTILINE)


FIELD_PATTERN = build_field_regex()


# =========================
# 2) æ ¸å¿ƒæŠ½å–
# =========================

def split_sections(text: str) -> Dict[str, str]:
    """ä»¥æ¬„ä½éŒ¨é»åˆ‡æ®µï¼Œå›å‚³ã€å‘½ä¸­çš„ä¸­æ–‡æ¬„ä½åã€â†’ã€å…§å®¹ã€çš„å°ç…§è¡¨ã€‚"""
    matches = list(FIELD_PATTERN.finditer(text))
    if not matches:
        return {}

    sections: Dict[str, str] = {}
    for i, m in enumerate(matches):
        start = m.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        field_name = m.group("field")
        after = m.group("after").strip()
        block = (after + "\n" + text[start:end]).strip() if after else text[start:end].strip()
        block = block.strip()
        # åŒåæ¬„ä½å¤šæ¬¡å‡ºç¾å°±ä¸²æ¥
        if field_name in sections and sections[field_name]:
            sections[field_name] = (sections[field_name] + "\n" + block).strip()
        else:
            sections[field_name] = block
    return sections


def canonicalize_keys(sections: Dict[str, str]) -> Dict[str, str]:
    """å°‡ä¸­æ–‡æ¬„ä½åè½‰æˆæ¨™æº–éµï¼ˆrecipient/doc_no/date/subject/body/...ï¼‰ã€‚"""
    out: Dict[str, str] = {}
    for canon, aliases in FIELD_ALIASES.items():
        for a in aliases:
            if a in sections and sections[a].strip():
                out[canon] = sections[a].strip()
                break
    return out


def _span_of_field(field_cn: str, text: str) -> Optional[Tuple[int, int]]:
    """å–å¾—æ–‡æœ¬ä¸­æŸä¸­æ–‡æ¬„ä½åå°æ‡‰çš„å…§å®¹ç¯„åœï¼ˆè¡Œå°¾åˆ°ä¸‹ä¸€æ¬„ä½æˆ–æ–‡æœ«ï¼‰ã€‚"""
    pat = re.compile(r"^" + re.escape(field_cn) + r"\s*:?\s*(.*)$", flags=re.MULTILINE)
    m = pat.search(text)
    if not m:
        return None
    start = m.end()
    nxt = FIELD_PATTERN.search(text, pos=start)
    end = nxt.start() if nxt else len(text)
    return (start, end)


def _middle_block(text: str) -> str:
    """ä¿åº•ï¼šå–ä¸­é–“ 60% å…§å®¹ä½œç‚ºæ­£æ–‡å€™é¸ï¼ˆå»æ‰é ­å°¾å™ªéŸ³ï¼‰ã€‚"""
    n = len(text)
    if n == 0:
        return ""
    return text[int(n * 0.2): int(n * 0.8)]


def heuristic_body(text: str, sections_raw: Dict[str, str], sections: Dict[str, str]) -> str:
    """
    æ­£æ–‡å›é€€ç­–ç•¥ï¼š
      1) è‹¥æœ‰ã€Œèªªæ˜/å…§æ–‡/æ­£æ–‡/æœ¬æ–‡ã€å‰‡ç›´æ¥å–ã€‚
      2) å¦å‰‡å–ã€Œä¸»æ—¨ã€ä¹‹å¾Œ â†’ ã€Œé™„ä»¶/å‰¯æœ¬/æ‰¿è¾¦ã€ä¹‹å‰ã€‚
      3) å†ä¸è¡Œï¼šåµæ¸¬æ¢åˆ—èµ·æ‰‹å¼ï¼ˆã€Œä¸€ã€äºŒã€ä¸‰ã€ã€ï¼‰ã€‚
      4) æœ€å¾Œä¿åº•å–ä¸­æ®µã€‚
    """
    if "body" in sections and sections["body"].strip():
        return sections["body"].strip()

    tail_markers = FIELD_ALIASES["attachment"] + FIELD_ALIASES["cc"] + FIELD_ALIASES["contact"]

    # æœ‰ä¸»æ—¨æ™‚ï¼šä¸»æ—¨ä¹‹å¾Œ â†’ æœ€è¿‘å°¾æ¨™ä¹‹å‰
    subject_span = _span_of_field("ä¸»æ—¨", text)
    if subject_span:
        start = subject_span[1]
        tail_positions = []
        for t in tail_markers:
            sp = _span_of_field(t, text)
            if sp and sp[0] > start:
                tail_positions.append(sp[0])
        end = min(tail_positions) if tail_positions else len(text)
        body = text[start:end].strip()
        # ç æ‰æ„å¤–æ®˜ç•™çš„æ¬„ä½è¡Œ
        body = re.sub(FIELD_PATTERN, "", body).strip()
        if body:
            return body

    # æ²’ä¸»æ—¨æˆ–ä¸Šé¢æŠ“ä¸åˆ°ï¼šæ‰¾æ¢åˆ—èµ·æ‰‹å¼
    m = re.search(r"^[ï¼ˆ(]?(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å)[)ï¼‰]?[ã€.ï¼]", text, flags=re.MULTILINE)
    if m:
        return text[m.start():].strip()

    # ä¿åº•
    return _middle_block(text).strip()


def extract_body_from_txt(txt_path: str, encoding: str = "utf-8") -> Dict[str, object]:
    """è®€å–å–®ä¸€ .txt æª”ä¸¦æŠ½å–æ¬„ä½ã€‚"""
    with open(txt_path, "r", encoding=encoding, errors="ignore") as f:
        raw = f.read()
    norm = normalize_text(raw)
    sections_raw = split_sections(norm)
    sections = canonicalize_keys(sections_raw)
    body = heuristic_body(norm, sections_raw, sections)

    return {
        "subject": sections.get("subject", ""),
        "body": body,
        "attachment": sections.get("attachment", ""),
        "meta": {
            "recipient": sections.get("recipient", ""),
            "doc_no": sections.get("doc_no", ""),
            "date": sections.get("date", ""),
            "priority": sections.get("priority", ""),
            "security": sections.get("security", ""),
        }
    }


# =========================
# 3) æ‰¹æ¬¡è™•ç†èˆ‡ CLI
# =========================

def process_all(
    input_root: Path,
    output_root: Path,
    only_folders: Optional[List[str]] = None,
    filename_contains: Optional[str] = None,
    encoding: str = "utf-8"
) -> None:
    """
    éæ­· input_root ä¸‹æ‰€æœ‰å­è³‡æ–™å¤¾èˆ‡ .txtï¼Œè¼¸å‡ºæ¯ä¸€é¡åˆ¥åˆ° output_root/<é¡åˆ¥>.jsonl
    - only_folders: é™å®šåªè™•ç†é€™äº›å­è³‡æ–™å¤¾ï¼ˆåç¨±å®Œå…¨æ¯”å°ï¼‰
    - filename_contains: åªè™•ç†æª”åä¸­åŒ…å«æ­¤é—œéµå­—çš„ .txt
    """
    output_root.mkdir(exist_ok=True, parents=True)

    # æ”¶é›†å­è³‡æ–™å¤¾
    folders = [p for p in input_root.iterdir() if p.is_dir()]
    if only_folders:
        target_set = set(only_folders)
        folders = [p for p in folders if p.name in target_set]

    if not folders:
        print(f"âš ï¸ æ‰¾ä¸åˆ°å­è³‡æ–™å¤¾å¯è™•ç†ï¼ˆroot: {input_root}ï¼‰")
        return

    for folder in sorted(folders, key=lambda p: p.name):
        category = folder.name
        out_file = output_root / f"{category}.jsonl"

        txt_files = sorted(folder.glob("*.txt"))
        if filename_contains:
            txt_files = [p for p in txt_files if filename_contains in p.name]

        if not txt_files:
            print(f"â„¹ï¸ é¡åˆ¥ã€Œ{category}ã€æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„ .txt æª”ï¼Œç•¥éã€‚")
            continue

        count_ok, count_err = 0, 0
        with open(out_file, "w", encoding="utf-8") as fout:
            for txt_path in txt_files:
                try:
                    result = extract_body_from_txt(str(txt_path), encoding=encoding)
                    result["filename"] = txt_path.name
                    result["category"] = category
                    # è¼¸å‡º JSON ä¸€è¡Œ
                    fout.write(json.dumps(result, ensure_ascii=False) + "\n")
                    count_ok += 1
                except Exception as e:
                    print(f"âŒ è™•ç†å¤±æ•—ï¼š{txt_path} -> {e}")
                    count_err += 1
        print(f"âœ… é¡åˆ¥ã€Œ{category}ã€å®Œæˆï¼šæˆåŠŸ {count_ok} ç­†ï¼Œå¤±æ•— {count_err} ç­† -> {out_file}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="æŠ½å– OCR å…¬æ–‡å…§æ–‡èˆ‡æ¬„ä½ï¼ˆæ‰¹æ¬¡è™•ç†è³‡æ–™å¤¾ï¼‰"
    )
    parser.add_argument(
        "--input-root", type=str, default="output",
        help="è¼¸å…¥æ ¹è³‡æ–™å¤¾ï¼ˆåº•ä¸‹æ¯å€‹å­è³‡æ–™å¤¾æ˜¯ä¸€ç¨®é¡åˆ¥ï¼‰"
    )
    parser.add_argument(
        "--output-root", type=str, default="parsed",
        help="è¼¸å‡ºæ ¹è³‡æ–™å¤¾ï¼ˆæ¯é¡è¼¸å‡ºä¸€å€‹ <é¡åˆ¥>.jsonlï¼‰"
    )
    parser.add_argument(
        "--folders", nargs="*", default=None,
        help="åƒ…è™•ç†æŒ‡å®šå­è³‡æ–™å¤¾åç¨±ï¼ˆå¯å¤šå€‹ï¼‰ï¼Œé è¨­è™•ç†å…¨éƒ¨"
    )
    parser.add_argument(
        "--filename-contains", type=str, default=None,
        help="åƒ…è™•ç†æª”ååŒ…å«æ­¤é—œéµå­—çš„ .txt"
    )
    parser.add_argument(
        "--encoding", type=str, default="utf-8",
        help="è¼¸å…¥æª”æ¡ˆç·¨ç¢¼ï¼ˆé è¨­ utf-8ï¼‰"
    )
    return parser.parse_args()


def main():
    args = parse_args()
    input_root = Path(args.input_root)
    output_root = Path(args.output_root)

    if not input_root.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥è³‡æ–™å¤¾ï¼š{input_root.resolve()}")
        return

    print(f"ğŸš€ é–‹å§‹è™•ç†ï¼šinput_root={input_root.resolve()}  ->  output_root={output_root.resolve()}")
    if args.folders:
        print(f"   åªè™•ç†å­è³‡æ–™å¤¾ï¼š{', '.join(args.folders)}")
    if args.filename_contains:
        print(f"   åªè™•ç†æª”ååŒ…å«ï¼š{args.filename_contains}")
    process_all(
        input_root=input_root,
        output_root=output_root,
        only_folders=args.folders,
        filename_contains=args.filename_contains,
        encoding=args.encoding
    )
    print("ğŸ‰ å…¨éƒ¨å®Œæˆã€‚")


if __name__ == "__main__":
    main()
